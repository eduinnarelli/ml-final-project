{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nbsvm_baseline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uUS9VoK3KgVp"},"source":["## **Projeto Final**: Aprendizado de Máquina MC886\n","\n","# **Detecção de Comentários Tóxicos em Português do Brasil:** Baseline com NBSVM\n","\n","Universidade de Campinas (UNICAMP), Instituto da Computação (IC)\n","\n","Prof. Sandra Avila, 2s2020\n","\n","**Grupo**:\n","- Eduardo Barros Innarelli (170161)\n","- João Pedro Congio Martins (176117)\n","- Pedro Alan Tapia Ramos (185531)"]},{"cell_type":"markdown","metadata":{"id":"0dFLd3y-JCRn"},"source":["---\n","\n","Se você estiver usando o Google Colab, conecte com o Google Drive para ter acesso aos datasets:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YmMe0CYJWCI","executionInfo":{"status":"ok","timestamp":1610473619619,"user_tz":180,"elapsed":25859,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}},"outputId":"8e22a29e-1647-4aef-9191-b17a1dcfce86"},"source":["from google.colab import drive\n","\n","# Isso solicitará autorização\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ArrMmG3Josa","executionInfo":{"status":"ok","timestamp":1610473621275,"user_tz":180,"elapsed":1646,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}},"outputId":"a91f746f-7022-4d83-b75b-169e8530f513"},"source":["# Vá até a pasta do projeto (depende da onde está salva no seu Drive)\n","% cd '/content/drive/My Drive/[MC886] Projeto Final/'\n","! ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/[MC886] Projeto Final\n","bertimbau.ipynb\t\t docs\t     nbsvm_baseline.ipynb  __pycache__\n","build_nbsvm_pipeline.py  model_save  nbsvm-model.joblib    split_dataset.py\n","data\t\t\t nbsvm\t     pretrained-bert\t   tokenizer.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZCWjDMACJ3uh"},"source":["---\n","\n","## **Dependências**"]},{"cell_type":"code","metadata":{"id":"D4K9QPscN7mq","executionInfo":{"status":"ok","timestamp":1610473728453,"user_tz":180,"elapsed":6938,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}}},"source":["# Para análise e manipulação de dados\n","import pandas as pd\n","\n","# Produto cartesiano\n","from itertools import product\n","\n","# Classe que permite treinar o modelo com grid search\n","from sklearn.model_selection import GridSearchCV\n","\n","# Função que divide os dados em treino e teste\n","from split_dataset import split_dataset\n","\n","# Função que constrói o pipeline de vetorização + treinamento\n","from build_nbsvm_pipeline import build_pipeline\n","\n","# Classe que controla como os textos serão tokenizados \n","from tokenizer import Tokenizer\n","\n","# Para salvar e carregar o modelo em um arquivo \n","from nbsvm.saving import save_model, load_model\n","\n","# Funções que computam métricas\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Para visualização dos dados\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zt1ZZfHdLKSd"},"source":["O tokenizador usa as stopwords do NLTK para aplicar stemização nos textos."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLctIU1hLNgN","executionInfo":{"status":"ok","timestamp":1610473733454,"user_tz":180,"elapsed":1034,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}},"outputId":"b6cb7a2f-41da-4d88-9590-d3052df0200b"},"source":["# Baixar stopwords do NLTK\n","import nltk\n","nltk.download('stopwords')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Ba1L-ccuNwmx"},"source":["## **Pré-processamento**\n","\n","Usamos a mesma estratégia do artigo base de agregar as classes de ofensa em uma única e classificar um tweet como tóxico se ao menos uma pessoa o anotou como tal em alguma das categorias.\n","\n","> \"In this paper, we consider the least restrictive case,\n","where if at least one annotator marked any offence\n","category in an example, the example is positive for\n","toxicity. Likewise, if a tweet was not tagged in any\n","of these categories, it is considered non-toxic. We\n","believe that it is essential that if any person feels\n","uncomfortable with a post, it should be flagged as\n","having a certain degree of toxicity. Therefore, a\n","model built with this data must be able to identify\n","offensive posts, even for a specific group of people.\"\n"]},{"cell_type":"code","metadata":{"id":"U9qmt7AyK5wV","executionInfo":{"status":"ok","timestamp":1610473739160,"user_tz":180,"elapsed":1647,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}}},"source":["# Carregar dataset\n","told_br = pd.read_csv(\"data/ToLD-BR.csv\")\n","\n","# Agregar classes de comentários ofensivos\n","told_br['toxic'] = told_br[['homophobia', \n","                            'obscene',\n","                            'insult',\n","                            'misogyny',\n","                            'xenophobia']].agg('sum', axis=1)\n","\n","# Converter em classificação binária (1 é tóxico, 0 não é tóxico)\n","told_br.loc[told_br['toxic'] >= 1, 'toxic'] = 1"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZuaueuh8-SV"},"source":["Agora, dividimos o dataset em treino e teste. Não separamos um conjunto de validação pois utilizaremos um método do `scikit-learn` que já aplica validação cruzada.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gppKFOAv8fPV","executionInfo":{"status":"ok","timestamp":1610473741739,"user_tz":180,"elapsed":970,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}},"outputId":"e2545e87-e52d-473a-8d36-aabb51e80ded"},"source":["# Dividir dataset em treino e teste\n","train, test, _ = split_dataset(told_br,\n","                               test_size=0.1,\n","                               valid_size=0.0,\n","                               target_col_name='toxic',\n","                               random_state=42)\n","\n","# Os conjuntos serão enviados para as funções do scikit-learn como arrays numpy\n","X_train, y_train = train['text'].values, train['toxic'].values\n","X_test, y_test = test['text'].values, test['toxic'].values"],"execution_count":6,"outputs":[{"output_type":"stream","text":["-->  Train size: 18900\n","-->  Valid size: 0\n","-->  Test size: 2100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mTwzBbAO_hXm"},"source":["## **Treinamento**\n","\n","Instanciamos o pipeline de treinamento do modelo NBSVM, que transforma a entrada do modelo em uma matriz documento-termo com o `TfidfVectorizer`.  "]},{"cell_type":"code","metadata":{"id":"xIZcjcqhG2m-"},"source":["# Definir alguns hiperparâmetros de base para o pipeline\n","base_hparams = {'do_stem': True,\n","                'min_word_size': 4,\n","                'do_lower': True,\n","                'min_df': 0.1,\n","                'max_df': 0.85,\n","                'C': 1,\n","                'dual': True,\n","                'solver': 'liblinear',\n","                'max_iter': 1000}\n","\n","# Instanciar pipeline\n","pipeline = build_pipeline(base_hparams)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RKA86EHHQPN"},"source":["Para treinar, adotaremos a técnica de otimização de hiperparâmetros intitulada grid search. O modelo será treinado com todas as possíveis combinações de hiperparâmetros definidos em `grid_params`, tal que o melhor modelo encontrado é salvo na classe. O método do `scikit-learn` também aplica validação cruzada, estratégia que ajuda a avaliar se o modelo generaliza bem."]},{"cell_type":"code","metadata":{"id":"GEdA35FGHPdH"},"source":["# Salvar todas as possíveis configurações do tokenizador\n","tokenizer_configs = []\n","for (do_stem, min_word_size) in product([True, False],  [3, 4, 5]):\n","    tokenizer_configs.append(\n","        Tokenizer(do_stem=do_stem, min_word_size=min_word_size).tokenize\n","    )\n","\n","# Hiperparâmetros que serão otimizados\n","grid_params = {'vectorizer__lowercase': [True, False],\n","               'vectorizer__tokenizer': tokenizer_configs,\n","               'vectorizer__min_df': [3, 5, 10, 20],\n","               'vectorizer__max_df': [0.5, 0.65, 0.8, 0.95],\n","               'nbsvm__C': [1e-1, 1, 4, 10]}\n","\n","# Grid search com validação cruzada\n","search = GridSearchCV(pipeline,\n","                      grid_params,\n","                      cv=5,\n","                      verbose=10,\n","                      error_score='raise')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWZmw2tII0mp"},"source":["Como são muitas combinações possíveis, o treinamento pode demorar bastante.\n","\n","**ATENÇÃO:** evite rodar esta célula caso o modelo já tenha sido treinado! Carregue do arquivo nas próximas células ou re-treine com os melhores hiperparâmetros encontrados. "]},{"cell_type":"code","metadata":{"id":"p7RrFTSCI3Vh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606252475000,"user_tz":180,"elapsed":8768484,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}},"outputId":"6fed3d08-830a-43f4-e0a5-6fc568621034"},"source":["# Treinar com grid search\n","search.fit(X_train, y_train)\n","print('Grid search finalizado!')\n","print('--> Melhor score:', search.best_score_)\n","print('--> Melhores hiperparâmetros:', search.best_params_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 5 folds for each of 768 candidates, totalling 3840 fits\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.695, total=  11.5s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.5s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.691, total=  11.5s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   23.0s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.690, total=  11.4s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   34.4s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.683, total=  11.4s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   45.8s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.681, total=  11.6s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   57.4s remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.664, total=  11.3s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.1min remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.668, total=  11.2s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  1.3min remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.654, total=  11.3s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  1.5min remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["[CV]  nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.664, total=  11.2s\n","[CV] nbsvm__C=0.1, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  1.7min remaining:    0.0s\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.724, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.719, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.707, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.707, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.696, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.642, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.644, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.704, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.705, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.729, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.695, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.640, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.648, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.719, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.712, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.9s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.637, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.641, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.616, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.699, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.730, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.682, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.674, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.672, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.643, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.647, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.708, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.8s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.642, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.644, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.704, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.729, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.695, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.640, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.648, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.705, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.712, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.637, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.641, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.616, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.677, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.730, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.682, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.674, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.672, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.643, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.647, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.708, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.642, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.644, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.704, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.729, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.8s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.695, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.640, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.648, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.705, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.712, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.637, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.641, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.616, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.677, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.730, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.682, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.674, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.672, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.643, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.647, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.9s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.708, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.642, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.644, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.704, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.729, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.695, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.640, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.648, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.705, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.712, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.637, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.641, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.616, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.677, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.730, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.682, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.674, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.672, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.643, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.647, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.708, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.690, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.642, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.644, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.704, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.729, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.696, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.695, total=  11.6s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.640, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.648, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.705, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.712, total=  11.7s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.678, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.693, total=  11.5s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.4s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.637, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.641, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.616, total=  11.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.3s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.1s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.0s\n","[CV] nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=1, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.677, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.8s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.8s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.740, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.736, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.703, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.719, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.717, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.692, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.740, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.736, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.703, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.719, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.717, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.692, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.740, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.736, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.703, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.719, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.717, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  12.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.740, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.736, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.720, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.721, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.703, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.700, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.719, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.702, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.717, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.733, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  12.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.716, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.712, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.685, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.681, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.733, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.716, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.712, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.685, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.681, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.733, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.716, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.712, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.685, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.681, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.734, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.733, total=  11.7s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.679, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.675, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.631, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.733, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.718, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.680, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.677, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.630, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.614, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.709, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.708, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.727, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.732, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.676, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.632, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.639, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.613, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.727, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.734, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.720, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.716, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.714, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.701, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.735, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.6s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.669, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.687, total=  11.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.636, total=  11.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  10.9s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.626, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.620, total=  11.0s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.719, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.712, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.4s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.685, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.681, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.1s\n","[CV] nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=4, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.1s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.1s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.738, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.738, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  12.0s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.1s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.738, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.0s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.738, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.732, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.730, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.728, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  12.0s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.731, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.713, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.705, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.715, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.711, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.698, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.717, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.691, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.693, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.696, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.694, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.697, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=True, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.679, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  12.0s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.713, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.699, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.684, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.688, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.676, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.684, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.5, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  12.0s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.713, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.699, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.684, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.688, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.676, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.684, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.65, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.713, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.699, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.684, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.688, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.676, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.684, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.8, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.9s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.667, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.661, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.624, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.1s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.1s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.628, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.621, total=  11.1s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.735, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.718, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.726, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.707, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.700, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.710, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.691, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.686, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.688, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=3, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.725, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.724, total=  11.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.714, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.723, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.666, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.683, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.660, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.673, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.671, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.609, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.625, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.725, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.722, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.704, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.706, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.687, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.685, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.690, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=5, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.726, total=  11.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.728, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.722, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.719, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.718, total=  11.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.685, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.662, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.681, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.665, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.634, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.612, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.629, total=  11.1s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.619, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.721, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.723, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.729, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.713, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.699, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.712, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.703, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.682, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.699, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.689, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.692, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=10, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.683, total=   1.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.721, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.731, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.720, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.717, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:3>>, score=0.716, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.663, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.688, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.656, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.684, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:4>>, score=0.659, total=  11.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.622, total=  11.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.633, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.607, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.627, total=  11.2s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:True min_word_size:5>>, score=0.618, total=  11.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.714, total=   1.6s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.8s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.715, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.724, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>, score=0.716, total=   1.7s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.684, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.701, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.688, total=   1.5s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.702, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:4>>, score=0.698, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.676, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.693, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.684, total=   1.4s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.695, total=   1.3s\n","[CV] nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>> \n","[CV]  nbsvm__C=10, vectorizer__lowercase=False, vectorizer__max_df=0.95, vectorizer__min_df=20, vectorizer__tokenizer=<bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:5>>, score=0.678, total=   1.3s\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=1)]: Done 3840 out of 3840 | elapsed: 406.0min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Grid search finalizado!\n","--> Melhor score: 0.7319642857142858\n","--> Melhores hiperparâmetros: {'nbsvm__C': 4, 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__tokenizer': <bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ghaaLmSQctZv"},"source":["Os hiperparâmetros que retornaram o melhor modelo foram:\n","\n","\n","\n","```\n","{\n","  'nbsvm__C': 4, \n","  'vectorizer__lowercase': True, \n","  'vectorizer__max_df': 0.5, \n","  'vectorizer__min_df': 3, \n","  'vectorizer__tokenizer': <bound method Tokenizer.tokenize of <Tokenizer do_stem:False min_word_size:3>>\n","}\n","```\n","\n","\n","\n","Salvamos o melhor modelo em um arquivo `joblib` para não precisar re-treinar."]},{"cell_type":"code","metadata":{"id":"V1Bx_Zltc3Ij"},"source":["# Salvar modelo\n","save_model('nbsvm-model.joblib', search.best_estimator_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LcczPbtwLjg4"},"source":["## **Avaliação**\n","\n","Primeiro, carregamos o modelo salvo no arquivo `nbsvm-model.joblib`."]},{"cell_type":"code","metadata":{"id":"KEih29YEMwOl","executionInfo":{"status":"ok","timestamp":1610473763789,"user_tz":180,"elapsed":1931,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}}},"source":["model = load_model('nbsvm-model.joblib')"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hImqgeWnKgrL"},"source":["Vamos avaliar como o modelo generaliza pro conjunto de testes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-5PlqpTJP0Z","executionInfo":{"status":"ok","timestamp":1610473768254,"user_tz":180,"elapsed":860,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}},"outputId":"995aaeb7-db71-4fcb-9d7c-d42f65654904"},"source":["# Predizer no conjunto de testes\n","preds = [pred for pred in model.predict(X_test)]\n","\n","# Mostrar métricas\n","print(classification_report(y_test, preds))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.73      0.80      0.77      1179\n","         1.0       0.71      0.62      0.66       921\n","\n","    accuracy                           0.72      2100\n","   macro avg       0.72      0.71      0.71      2100\n","weighted avg       0.72      0.72      0.72      2100\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NkycIeM6P8Mi"},"source":["Finalmente, plotamos a matriz de confusão para termos uma visão melhor dos acertos e erros do modelo treinado."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"9UTCt464O-QL","executionInfo":{"status":"ok","timestamp":1610473777667,"user_tz":180,"elapsed":1654,"user":{"displayName":"Eduardo Barros Innarelli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-HPBu7P07dv37oyEQB7d3kQqc3oZtuG58DW9x=s64","userId":"06818377933646517062"}},"outputId":"69796fef-d071-4538-a4de-029b741f4114"},"source":["# Construir matriz de confusão com scikit-learn\n","cmatrix = confusion_matrix(y_test, preds)\n","\n","# Salvar matriz como um dataframe do pandas\n","labels = ['inofensivo', 'tóxico']\n","df_cm = pd.DataFrame(cmatrix, columns=labels, index=labels)\n","df_cm.index.name = 'Real'\n","df_cm.columns.name = 'Previsto'\n","\n","# Plotar como um mapa de calor\n","plt.figure(figsize = (10,7))\n","sns.set(font_scale=1.2)\n","sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 10}, fmt='g');"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}